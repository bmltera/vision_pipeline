{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Imports\n",
    "import os, time, json, urllib.request, shutil\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ultralytics import YOLO  # YOLOv* and YOLO‑World\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 1: Setup paths ─────────────────────────────────────────────────────────\n",
    "yolov11_weight   = \"weights/bestyolo.pt\"\n",
    "yoloworld_weight = \"weights/yolov8l-worldv2.pt\"   # ← download and point here\n",
    "owlv2_weight     = \"weights/owlv2.pt\"       # ← download and point here\n",
    "\n",
    "image_folder      = \"dataset/final/images\"\n",
    "yolo_annot_folder = \"dataset/final/labels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 2: Data preprocess ────────────────────────────────────────────────────\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "# gather all image paths\n",
    "image_paths = sorted([\n",
    "    os.path.join(image_folder, f)\n",
    "    for f in os.listdir(image_folder)\n",
    "    if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "])\n",
    "\n",
    "# build corresponding YOLO-label paths\n",
    "label_paths = [\n",
    "    os.path.join(yolo_annot_folder, os.path.splitext(os.path.basename(p))[0] + \".txt\")\n",
    "    for p in image_paths\n",
    "]\n",
    "\n",
    "# YOLO DataFrame\n",
    "yolo_df = pd.DataFrame({\n",
    "    \"image_path\": image_paths,\n",
    "    \"label_path\": label_paths\n",
    "})\n",
    "\n",
    "# Build a COCO‐style DataFrame from YOLO-format labels\n",
    "coco_rows = []\n",
    "for img_path, lbl_path in zip(image_paths, label_paths):\n",
    "    img = Image.open(img_path)\n",
    "    w, h = img.size\n",
    "    with open(lbl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            cls, xc, yc, bw, bh = map(float, line.split())\n",
    "            # convert normalized center→pixel xywh\n",
    "            x1 = (xc - bw/2) * w\n",
    "            y1 = (yc - bh/2) * h\n",
    "            coco_rows.append({\n",
    "                \"image_path\": img_path,\n",
    "                \"category_id\": int(cls),\n",
    "                \"bbox\": [x1, y1, bw * w, bh * h]\n",
    "            })\n",
    "\n",
    "coco_df = pd.DataFrame(coco_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell X: GPU memory used in GB ─────────────────────────────────────────────\n",
    "import subprocess\n",
    "\n",
    "def get_gpu_mem_gb():\n",
    "    \"\"\"\n",
    "    Returns the currently used GPU memory (first GPU) in gigabytes.\n",
    "    \"\"\"\n",
    "    # query only “memory.used” (in MiB)\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\",\n",
    "        \"--query-gpu=memory.used\",\n",
    "        \"--format=csv,noheader,nounits\"\n",
    "    ])\n",
    "    used_mib = float(out.decode().strip().split()[0])\n",
    "    used_gb  = used_mib / 1024.0\n",
    "    return used_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'C:\\\\Users\\\\chrap\\\\Documents\\\\code\\\\vision_pipeline\\\\model_evaluation', 'train': 'dataset/final/images', 'val': 'dataset/final/images', 'nc': 2, 'names': ['graffiti', 'trash']}\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 3: Create data.yaml with absolute path ───────────────────────────────\n",
    "import yaml\n",
    "\n",
    "# ← hard-code your project root here:\n",
    "project_root = r\"C:\\Users\\chrap\\Documents\\code\\vision_pipeline\\model_evaluation\"\n",
    "\n",
    "# number of classes (as a pure Python int)\n",
    "num_classes = int(coco_df.category_id.max()) + 1\n",
    "\n",
    "data_cfg = {\n",
    "    'path':  project_root,           # absolute root\n",
    "    'train': 'dataset/final/images',       # relative to path/\n",
    "    'val':   'dataset/final/images',       # same for validation\n",
    "    'nc':    num_classes,            # number of classes\n",
    "    #'names': [str(i) for i in range(num_classes)]\n",
    "    'names': [\"graffiti\", \"trash\"]\n",
    "}\n",
    "\n",
    "with open('data.yaml', 'w') as f:\n",
    "    yaml.safe_dump(data_cfg, f, sort_keys=False)\n",
    "\n",
    "# Optional: verify contents\n",
    "print(yaml.safe_load(open(\"data.yaml\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 89\n",
      "Number of unique images in coco_df (with at least one GT box): 52\n",
      "Images missing a .txt label file: []\n",
      "Label files present but empty: ['dataset/final/labels\\\\clean_blank10.txt', 'dataset/final/labels\\\\clean_blank12.txt', 'dataset/final/labels\\\\clean_blank13.txt', 'dataset/final/labels\\\\clean_blank16.txt', 'dataset/final/labels\\\\clean_blank19.txt', 'dataset/final/labels\\\\clean_blank23.txt', 'dataset/final/labels\\\\clean_blank25.txt', 'dataset/final/labels\\\\clean_blank27.txt', 'dataset/final/labels\\\\clean_blank28.txt', 'dataset/final/labels\\\\clean_blank29.txt', 'dataset/final/labels\\\\clean_blank30.txt', 'dataset/final/labels\\\\clean_blank31.txt', 'dataset/final/labels\\\\clean_blank32.txt', 'dataset/final/labels\\\\clean_blank33.txt', 'dataset/final/labels\\\\clean_blank35.txt', 'dataset/final/labels\\\\clean_blank37.txt', 'dataset/final/labels\\\\clean_blank38.txt', 'dataset/final/labels\\\\clean_blank39.txt', 'dataset/final/labels\\\\clean_blank40.txt', 'dataset/final/labels\\\\clean_blank41.txt', 'dataset/final/labels\\\\clean_blank43.txt', 'dataset/final/labels\\\\clean_blank44.txt', 'dataset/final/labels\\\\clean_blank45.txt', 'dataset/final/labels\\\\clean_blank46.txt', 'dataset/final/labels\\\\clean_blank47.txt', 'dataset/final/labels\\\\clean_blank48.txt', 'dataset/final/labels\\\\clean_blank49.txt', 'dataset/final/labels\\\\clean_blank50.txt', 'dataset/final/labels\\\\clean_blank51.txt', 'dataset/final/labels\\\\clean_blank52.txt', 'dataset/final/labels\\\\clean_blank6.txt', 'dataset/final/labels\\\\grafi_11.txt', 'dataset/final/labels\\\\grafi_23.txt', 'dataset/final/labels\\\\grafi_45.txt', 'dataset/final/labels\\\\grafi_5.txt', 'dataset/final/labels\\\\grafi_6.txt', 'dataset/final/labels\\\\grafi_8.txt']\n",
      "\n",
      "yolo_df head:\n",
      "                                image_path  \\\n",
      "0  dataset/final/images\\clean_blank10.jpg   \n",
      "1  dataset/final/images\\clean_blank12.jpg   \n",
      "2  dataset/final/images\\clean_blank13.jpg   \n",
      "3  dataset/final/images\\clean_blank16.jpg   \n",
      "4  dataset/final/images\\clean_blank19.jpg   \n",
      "\n",
      "                               label_path  \n",
      "0  dataset/final/labels\\clean_blank10.txt  \n",
      "1  dataset/final/labels\\clean_blank12.txt  \n",
      "2  dataset/final/labels\\clean_blank13.txt  \n",
      "3  dataset/final/labels\\clean_blank16.txt  \n",
      "4  dataset/final/labels\\clean_blank19.txt  \n",
      "\n",
      "coco_df head:\n",
      "                           image_path  category_id  \\\n",
      "0   dataset/final/images\\grafi_1.jpg            1   \n",
      "1   dataset/final/images\\grafi_1.jpg            1   \n",
      "2   dataset/final/images\\grafi_1.jpg            1   \n",
      "3   dataset/final/images\\grafi_1.jpg            1   \n",
      "4  dataset/final/images\\grafi_10.jpg            1   \n",
      "\n",
      "                                                bbox  \n",
      "0  [1910.6997569999999, 658.4605875, 133.551214, ...  \n",
      "1  [1596.321512, 689.0693625000001, 61.209164, 61...  \n",
      "2  [1654.7493699999998, 658.4592825000001, 233.70...  \n",
      "3  [2080.409226, 675.1606725, 472.970152, 275.429...  \n",
      "4  [764.3797, 249.99014250000002, 72.10003, 51.50...  \n"
     ]
    }
   ],
   "source": [
    "# ─── Cell DEBUG: sanity-check your dataframes ─────────────────────────────────\n",
    "import os\n",
    "\n",
    "print(\"Number of images:\", len(yolo_df))\n",
    "print(\"Number of unique images in coco_df (with at least one GT box):\",\n",
    "      coco_df.image_path.nunique())\n",
    "\n",
    "# Check for any images where the label file doesn’t exist or is empty\n",
    "missing_lbl = [p for p, l in zip(yolo_df.image_path, yolo_df.label_path)\n",
    "               if not os.path.isfile(l)]\n",
    "empty_lbl   = [l for l in yolo_df.label_path if os.path.isfile(l) and os.path.getsize(l)==0]\n",
    "print(\"Images missing a .txt label file:\", missing_lbl)\n",
    "print(\"Label files present but empty:\", empty_lbl)\n",
    "\n",
    "# Show a few entries\n",
    "print(\"\\nyolo_df head:\\n\", yolo_df.head())\n",
    "print(\"\\ncoco_df head:\\n\", coco_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell 4: YOLOv11 inference ─────────────────────────────────────────────────\n",
    "import time\n",
    "import subprocess\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def infer_yolov11(df, weights, data_yaml='data.yaml'):\n",
    "    model = YOLO(weights)\n",
    "    _ = model(df.image_path.iloc[:1].tolist())  # warm up\n",
    "\n",
    "    start = time.time()\n",
    "    results = model.val(data=data_yaml, verbose=False)\n",
    "    end = time.time()\n",
    "\n",
    "    gpu_util   = get_gpu_mem_gb()\n",
    "    map50      = results.box.ap50        # was map50\n",
    "    map50_95   = results.box.ap          # was map50_95\n",
    "    avg_ms     = (end - start) * 1000 / len(df)\n",
    "\n",
    "    return gpu_util, map50, map50_95, avg_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_yoloworld_eval(df, weights, conf=0.30, save_vis=True, vis_dir=\"yolo_world_vis\"):\n",
    "    class_names = [\"graffiti\", \"trash\"]\n",
    "    model = YOLO(weights)\n",
    "    model.set_classes(class_names)\n",
    "    _ = model(df.image_path.iloc[:1].tolist())  # warmup\n",
    "\n",
    "    # inference\n",
    "    results = model.predict(source=df.image_path.tolist(), conf=conf, verbose=False)\n",
    "\n",
    "    # save visualizations\n",
    "    if save_vis:\n",
    "        os.makedirs(vis_dir, exist_ok=True)\n",
    "        for img_path, res in zip(df.image_path.tolist(), results):\n",
    "            im_arr = res.plot()\n",
    "            cv2.imwrite(os.path.join(vis_dir, os.path.basename(img_path)), im_arr)\n",
    "\n",
    "    # build preds & targets\n",
    "    preds, targets = [], []\n",
    "    for img_path, res in zip(df.image_path.tolist(), results):\n",
    "        boxes  = torch.tensor(res.boxes.xyxy.cpu())\n",
    "        scores = torch.tensor(res.boxes.conf.cpu())\n",
    "        labels = torch.tensor(res.boxes.cls.cpu().long())  # assume 0=graffiti, 1=trash\n",
    "        preds.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "\n",
    "        gt = coco_df[coco_df.image_path == img_path]\n",
    "        if not gt.empty:\n",
    "            xywh = torch.tensor(gt.bbox.tolist())\n",
    "            gt_boxes = torch.cat([xywh[:, :2], xywh[:, :2] + xywh[:, 2:]], dim=1)\n",
    "            gt_labels = torch.tensor(gt.category_id.tolist(), dtype=torch.int64)\n",
    "        else:\n",
    "            gt_boxes  = torch.zeros((0,4), dtype=torch.float32)\n",
    "            gt_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "        targets.append({\"boxes\": gt_boxes, \"labels\": gt_labels})\n",
    "\n",
    "    # evaluate\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "    metric.update(preds, targets)\n",
    "    m = metric.compute()\n",
    "\n",
    "    # GPU usage\n",
    "    out = subprocess.check_output([\"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"])\n",
    "    gpu_mem_gb = float(out.decode().strip()) / 1024.0\n",
    "    avg_ms = (time.time() - start) * 1000 / len(df)\n",
    "\n",
    "    return gpu_mem_gb, m[\"map_50\"].item(), m[\"map\"].item(), avg_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Cell X: OWLv2 inference & mAP for graffiti+trash on GPU with viz toggle ───\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def infer_owlv2(df,\n",
    "                model_id=\"google/owlv2-base-patch16-ensemble\",\n",
    "                text_labels=None,\n",
    "                conf_threshold=0.05,\n",
    "                save_vis=False,\n",
    "                vis_dir=\"owlv2_vis\"):\n",
    "    \"\"\"\n",
    "    Runs OWLv2 on df.image_path using GPU if available, computes open-vocab mAP vs. coco_df GT,\n",
    "    measures GPU mem (GB) and avg inference time (ms/image).\n",
    "    If save_vis=True, saves annotated images to vis_dir.\n",
    "    \"\"\"\n",
    "    # default to graffiti+trash\n",
    "    if text_labels is None:\n",
    "        text_labels = [\"graffiti\", \"trash\"]\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load processor & model\n",
    "    processor = Owlv2Processor.from_pretrained(model_id)\n",
    "    model     = Owlv2ForObjectDetection.from_pretrained(model_id).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # warm-up\n",
    "    img0 = Image.open(df.image_path.iloc[0]).convert(\"RGB\")\n",
    "    inputs = processor(text=[text_labels], images=img0, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs)\n",
    "\n",
    "    preds, targets = [], []\n",
    "    start = time.time()\n",
    "\n",
    "    for img_path in df.image_path.tolist():\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        inputs = processor(text=[text_labels], images=img, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # post-process\n",
    "        proc = processor.post_process_grounded_object_detection(\n",
    "            outputs=outputs,\n",
    "            threshold=conf_threshold,\n",
    "            target_sizes=torch.tensor([(img.height, img.width)]),\n",
    "            text_labels=text_labels\n",
    "        )[0]\n",
    "\n",
    "        # visualize if asked\n",
    "        if save_vis:\n",
    "            os.makedirs(vis_dir, exist_ok=True)\n",
    "            vis = np.array(img)[:, :, ::-1].copy()  # RGB→BGR\n",
    "            for box in proc[\"boxes\"]:\n",
    "                x1,y1,x2,y2 = map(int, box.tolist())\n",
    "                cv2.rectangle(vis, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "            cv2.imwrite(os.path.join(vis_dir, os.path.basename(img_path)), vis)\n",
    "\n",
    "        # build predictions\n",
    "        boxes  = proc[\"boxes\"].cpu()\n",
    "        scores = proc[\"scores\"].cpu()\n",
    "        labels = torch.tensor([text_labels.index(lbl) for lbl in proc[\"text_labels\"]],\n",
    "                              dtype=torch.int64)\n",
    "        preds.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "\n",
    "        # build targets\n",
    "        gt = coco_df[coco_df.image_path == img_path]\n",
    "        if not gt.empty:\n",
    "            xywh      = torch.tensor(gt.bbox.tolist())\n",
    "            x1y1      = xywh[:, :2]\n",
    "            wh        = xywh[:, 2:]\n",
    "            x2y2      = x1y1 + wh\n",
    "            gt_boxes  = torch.cat([x1y1, x2y2], dim=1)\n",
    "            gt_labels = torch.tensor(gt.category_id.tolist(), dtype=torch.int64)\n",
    "        else:\n",
    "            gt_boxes  = torch.zeros((0,4), dtype=torch.float32)\n",
    "            gt_labels = torch.zeros((0,),   dtype=torch.int64)\n",
    "        targets.append({\"boxes\": gt_boxes, \"labels\": gt_labels})\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # compute mAP\n",
    "    metric = MeanAveragePrecision(iou_type=\"bbox\")\n",
    "    metric.update(preds, targets)\n",
    "    mAPs     = metric.compute()\n",
    "    map50    = mAPs[\"map_50\"].item()\n",
    "    map50_95 = mAPs[\"map\"].item()\n",
    "\n",
    "    # GPU memory used in GB\n",
    "    out = subprocess.check_output([\n",
    "        \"nvidia-smi\", \"--query-gpu=memory.used\", \"--format=csv,noheader,nounits\"\n",
    "    ]).decode().strip()\n",
    "    gpu_mem_gb = float(out) / 1024.0\n",
    "\n",
    "    avg_ms = (end - start) * 1000 / len(df)\n",
    "\n",
    "    return gpu_mem_gb, map50, map50_95, avg_ms\n",
    "\n",
    "# Example:\n",
    "# gpu_gb, m50, m5095, avg_ms = infer_owlv2(\n",
    "#     yolo_df,\n",
    "#     text_labels=[\"graffiti\",\"trash\"],\n",
    "#     conf_threshold=0.1,\n",
    "#     save_vis=True,\n",
    "#     vis_dir=\"owlv2_multi_vis\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 (no detections), 23.0ms\n",
      "Speed: 1.7ms preprocess, 23.0ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Ultralytics 8.3.127  Python-3.10.16 torch-2.6.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 601.7187.5 MB/s, size: 45.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\chrap\\Documents\\code\\vision_pipeline\\model_evaluation\\dataset\\final\\labels... 89 images, 37 backgrounds, 0 corrupt: 100%|██████████| 89/89 [00:00<00:00, 1888.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\chrap\\Documents\\code\\vision_pipeline\\model_evaluation\\dataset\\final\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         89        133          0          0          0          0\n",
      "Speed: 0.9ms preprocess, 5.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val4\u001b[0m\n",
      "\n",
      "0: 320x640 (no detections), 17.4ms\n",
      "Speed: 1.1ms preprocess, 17.4ms inference, 0.7ms postprocess per image at shape (1, 3, 320, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chrap\\AppData\\Local\\Temp\\ipykernel_16028\\410530449.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  boxes = torch.tensor(res.boxes.xyxy.cpu())\n",
      "C:\\Users\\chrap\\AppData\\Local\\Temp\\ipykernel_16028\\410530449.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  scores = torch.tensor(res.boxes.conf.cpu())\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 6: Run all three and collect ─────────────────────────────────────────\n",
    "metrics = {}\n",
    "metrics[\"owlv2\"]    = infer_owlv2(yolo_df)\n",
    "metrics[\"yolov11\"]  = infer_yolov11(yolo_df,  yolov11_weight)\n",
    "metrics[\"yolo-world\"] = infer_yoloworld_eval(yolo_df, yoloworld_weight)\n",
    "\n",
    "# unpack into lists for plotting\n",
    "models     = list(metrics.keys())\n",
    "gpu_utils  = [metrics[m][0] for m in models]\n",
    "maps50     = [metrics[m][1] for m in models]\n",
    "maps50_95  = [metrics[m][2] for m in models]\n",
    "speeds_ms  = [metrics[m][3] for m in models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only length-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# sanitize metrics: replace None with 0.0, cast to float\u001b[39;00m\n\u001b[0;32m      6\u001b[0m gpu_utils_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m gpu_utils]\n\u001b[1;32m----> 7\u001b[0m maps50_clean    \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m maps50]\n\u001b[0;32m      8\u001b[0m maps50_95_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m maps50_95]\n\u001b[0;32m      9\u001b[0m speeds_ms_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m speeds_ms]\n",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# sanitize metrics: replace None with 0.0, cast to float\u001b[39;00m\n\u001b[0;32m      6\u001b[0m gpu_utils_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m gpu_utils]\n\u001b[1;32m----> 7\u001b[0m maps50_clean    \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m maps50]\n\u001b[0;32m      8\u001b[0m maps50_95_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m maps50_95]\n\u001b[0;32m      9\u001b[0m speeds_ms_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(x) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m speeds_ms]\n",
      "\u001b[1;31mTypeError\u001b[0m: only length-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# ─── Cell 7: Plot bar charts (with inline display, robust scalar conversion) ─────\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def to_scalar(x):\n",
    "    \"\"\"Convert x to a single float:\n",
    "    - None      → 0.0\n",
    "    - scalar    → float(x)\n",
    "    - array/list → float(first element) if possible\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return 0.0\n",
    "    # torch tensor?\n",
    "    try:\n",
    "        if hasattr(x, 'item'):\n",
    "            return float(x.item())\n",
    "    except:\n",
    "        pass\n",
    "    # numpy array or list/tuple\n",
    "    if isinstance(x, (list, tuple, np.ndarray)):\n",
    "        if len(x) > 0:\n",
    "            return float(x[0])\n",
    "        else:\n",
    "            return 0.0\n",
    "    # fallback\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "# sanitize metrics\n",
    "gpu_utils_clean  = [to_scalar(x) for x in gpu_utils]\n",
    "maps50_clean     = [to_scalar(x) for x in maps50]\n",
    "maps50_95_clean  = [to_scalar(x) for x in maps50_95]\n",
    "speeds_ms_clean  = [to_scalar(x) for x in speeds_ms]\n",
    "\n",
    "def plot_bar(vals, ylabel, title):\n",
    "    plt.figure()\n",
    "    bars = plt.bar(models, vals)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    for bar, v in zip(bars, vals):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2,\n",
    "                 bar.get_height(),\n",
    "                 f\"{v:.2f}\",\n",
    "                 ha='center', va='bottom')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# GPU memory utilization (% stays same name though it’s GB if you used get_gpu_mem_gb)\n",
    "plot_bar(gpu_utils_clean, \"GPU memory used (GB)\", \"GPU Memory Utilization\")\n",
    "\n",
    "# mAP@50\n",
    "plot_bar(maps50_clean, \"mAP@50\", \"mAP@50 Comparison\")\n",
    "\n",
    "# mAP@50–95\n",
    "plot_bar(maps50_95_clean, \"mAP@50–95\", \"mAP@50–95 Comparison\")\n",
    "\n",
    "# avg inference speed\n",
    "plot_bar(speeds_ms_clean, \"Avg. inference (ms/frame)\", \"Inference Speed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
